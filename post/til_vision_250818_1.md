---
title: "트랜스포머(Transformer) 아키텍처"
date: "2025-08-18"
keywords: ["TIL", "트랜스포머 모델"]
---

# 트랜스포머(Transformer) 아키텍처

## 트랜스포머의 등장 배경: RNN의 한계

기존의 순차 데이터 처리 모델인 RNN(LSTM, GRU 포함)은 다음과 같은 본질적인 한계를 가집니다.

- **순차 처리의 비효율성**: 입력을 하나씩 순서대로 처리해야 하므로, GPU를 활용한 **병렬 처리가 불가능**하여 훈련 속도가 느립니다.
- **장기 의존성 문제**: 시퀀스가 길어질수록 문장 앞부분의 정보가 뒤로 전달되기 어려워, 먼 단어 간의 관계를 학습하기 어렵습니다.
- **정보 손실**: 고정된 크기의 벡터(Context Vector)에 모든 문맥 정보를 압축하는 과정에서 정보 손실이 발생할 수 있습니다.

트랜스포머는 이러한 RNN의 한계를 극복하고, 입력 시퀀스 전체를 한 번에 처리하기 위해 **어텐션(Attention)** 메커니즘을 중심으로 설계된 모델입니다.

## 핵심 메커니즘: 셀프 어텐션 (Self-Attention)

- **개념**: 문장 내의 다른 모든 단어와의 관계(유사도)를 계산하여, 현재 단어의 의미를 표현할 때 어떤 단어에 더 집중(Attention)해야 할지를 결정하는 메커니즘입니다.
- **구성 요소**: 각 단어는 세 가지 벡터, **쿼리(Query, Q)**, **키(Key, K)**, **값(Value, V)**으로 표현됩니다.
  - **Q (Query)**: 현재 처리 중인 단어의 표현 (정보를 요청하는 주체)
  - **K (Key)**: 문장 내 모든 단어의 표현 (Q와 비교될 대상)
  - **V (Value)**: 각 단어가 실제로 담고 있는 정보
- **계산 과정**: 현재 단어의 Q 벡터와 모든 단어의 K 벡터 간의 유사도(Attention Score)를 계산하고, 이 점수를 Softmax 함수로 정규화하여 **어텐션 가중치(Attention Weight)**를 구합니다. 마지막으로 이 가중치를 각 단어의 V 벡터에 곱하여 모두 더함으로써, 문맥이 풍부하게 반영된 현재 단어의 최종 표현을 얻습니다.

## 멀티-헤드 어텐션 (Multi-Head Attention)

- **개념**: 하나의 셀프 어텐션을 병렬적으로 여러 개(Head) 수행하여, 다양한 관점의 정보를 동시에 포착하는 메커니즘입니다.
- **원리**: 예를 들어, 한 헤드는 문법적 관계를, 다른 헤드는 의미적 관계를 파악하는 등, 각 헤드가 서로 다른 종류의 관계에 집중하여 더 풍부한 표현을 학습할 수 있습니다.

## 트랜스포머의 주요 구조

### 1. 포지셔널 인코딩 (Positional Encoding)

트랜스포머는 문장 전체를 한 번에 처리하기 때문에, RNN과 달리 단어의 순서 정보를 자연스럽게 알지 못합니다. 따라서 각 단어의 **위치 정보**를 별도의 벡터로 만들어 입력 임베딩에 더해주는 과정이 필수적입니다.

### 2. 인코더 (Encoder)

- 입력 시퀀스의 문맥적 의미를 파악하여 풍부한 정보가 담긴 벡터 표현을 만듭니다.
- 여러 개의 동일한 블록으로 구성되며, 각 블록은 **멀티-헤드 셀프 어텐션**과 **피드 포워드 신경망(FFN)**으로 이루어져 있습니다.

### 3. 디코더 (Decoder)

- 인코더가 생성한 벡터 표현을 바탕으로, 출력 시퀀스(예: 번역된 문장)를 단어 하나씩 생성합니다.
- 디코더 블록은 인코더 블록의 두 가지 층에 더해, **크로스 어텐션(Cross-Attention)** 층을 추가로 가집니다.
- **크로스 어텐션**: 디코더가 출력 단어를 생성할 때, 인코더의 출력(입력 문장 전체의 정보)을 직접 참조하여 입력과 출력 간의 관계를 가장 잘 반영하도록 돕는 핵심적인 메커니즘입니다.

> **프롬프트 엔지니어링의 원리**: 트랜스포머의 어텐션 원리에 따라, 입력 문장에서 핵심 정보를 명확하고 구조적으로 제시할수록 모델은 중요한 부분에 더 높은 가중치를 부여하게 되고, 이는 더 정확하고 분명한 응답을 생성하는 결과로 이어집니다.
