---
title: "딥러닝의 시작, 인공 신경망"
date: "2025-08-06"
keywords: ["TIL", "딥러닝"]
---

# 딥러닝의 시작, 인공 신경망

## 인공 신경망 (Artificial Neural Network, ANN)

- **개념**: 인간의 뇌를 구성하는 신경세포(뉴런)의 작동 방식을 모방하여 만든 수학적 모델입니다. 수많은 뉴런(노드)들이 서로 연결되어 계층(Layer)을 이루고, 계층들이 모여 하나의 네트워크를 형성합니다.
- **기본 단위, 뉴런(Neuron)**: 하나의 뉴런은 그 자체로 간단한 **선형 모델**입니다. (y = w \* x + b)
  - 여러 개의 입력(x)을 받아 각각의 **가중치(w)**와 곱한 후, 모두 더하고 **편향(b)**을 더하여 하나의 출력 신호를 만듭니다.

## 퍼셉트론 (Perceptron)

- **정의**: 가장 단순한 형태의 인공 신경망으로, 다수의 입력을 받아 하나의 출력을 내보내는 구조입니다.
- **특징**: 신호를 '준다(1)' 또는 '주지 않는다(0)'만 결정할 뿐, 신호의 세기는 고려하지 않습니다. 이처럼 두 개의 클래스를 구분하는 선형 분류기 역할을 합니다.
- **한계**: 단층 퍼셉트론은 **선형(Linear)** 문제만 해결할 수 있습니다. 즉, 직선 하나로 데이터를 완벽하게 나눌 수 없는 XOR와 같은 비선형(Non-linear) 문제는 해결하지 못합니다.

## 다층 퍼셉트론 (Multi-Layer Perceptron, MLP)과 딥러닝

- **다층 구조**: 입력층(Input Layer)과 출력층(Output Layer) 사이에 하나 이상의 **은닉층(Hidden Layer)**을 추가하여 퍼셉트론을 여러 층으로 쌓은 구조입니다.
- **딥러닝**: 은닉층이 2개 이상인 깊은(Deep) 신경망을 **심층 신경망(Deep Neural Network, DNN)**이라 하며, 이를 이용한 학습 방식을 **딥러닝**이라고 합니다.
- **비선형 문제 해결**: 다층 구조와 비선형 활성 함수를 통해 단층 퍼셉트론의 한계를 극복하고 복잡한 비선형 문제를 해결할 수 있습니다.

## 활성 함수 (Activation Function)

- **역할**: 뉴런에서 계산된 출력 값을 그대로 다음 층으로 전달할지, 아니면 변환하여 전달할지를 결정하는 **비선형(non-linear) 함수**입니다.
- **필요성**: 만약 활성 함수가 없다면(또는 선형 함수라면), 신경망을 아무리 깊게 쌓아도 결국 하나의 선형 모델과 동일한 결과를 내게 됩니다. 활성 함수는 신경망에 **비선형성**을 부여하여 모델이 훨씬 복잡하고 다양한 패턴을 학습할 수 있도록 해주는 핵심 요소입니다.
- **예시**: 시그모이드(Sigmoid), 하이퍼볼릭 탄젠트(tanh), 렐루(ReLU) 등

## 신경망의 구조

- **입력층 (Input Layer)**: 데이터가 처음으로 들어오는 층. 입력층의 뉴런 개수는 데이터의 **피처(Feature) 개수**와 동일해야 하며, 사용자가 임의로 정할 수 없습니다.
- **은닉층 (Hidden Layer)**: 입력층과 출력층 사이에 위치한 모든 층. 은닉층의 개수(네트워크의 **깊이**)와 각 층의 뉴런 수(네트워크의 **너비**)는 사용자가 직접 설계하고 튜닝할 수 있는 **하이퍼파라미터**입니다.
- **출력층 (Output Layer)**: 모델의 최종 예측 결과를 출력하는 층. 출력층의 뉴런 개수는 해결하려는 문제의 **정답 형태(클래스 개수 등)**에 따라 결정되며, 사용자가 임의로 정할 수 없습니다.
