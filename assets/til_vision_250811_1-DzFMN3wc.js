const n=`---
title: "딥러닝 학습의 난관과 최적화 기법"
date: "2025-08-11"
keywords: ["TIL", "딥러닝"]
---

# 딥러닝 학습의 난관과 최적화 기법

## 딥러닝 학습의 필수 요소와 과제

- **학습의 필수 요소**: 딥러닝 학습은 예측과 정답의 차이를 정의하는 **손실 함수(Loss Function)**와, 그 손실을 줄여나가는 방법을 결정하는 **최적화 알고리즘(Optimizer)**을 반드시 필요로 합니다.
- **학습의 어려움**: 손실 함수의 표면은 매우 복잡하여, 학습 과정에서 다음과 같은 문제에 직면할 수 있습니다.
  - **협곡(Valley)**: 경사가 0은 아니지만 매우 완만하고 좁은 골짜기 형태의 구간. 이 구간에서는 최적점을 향해 나아가지 못하고 좌우로 진동하며 불필요한 학습 시간을 소모할 수 있습니다.
  - **안장점(Saddle Point)**: 특정 차원에서는 극대값이지만 다른 차원에서는 극소값인 지점. 기울기(미분값)가 0이 되어 학습이 멈추는 것처럼 보일 수 있습니다.

## 모멘텀 기반 최적화: 관성을 이용한 전진

- **SGD with Momentum**: 기본적인 경사 하강법(SGD)에 **관성(Momentum)**의 개념을 도입한 방식입니다. 이전 스텝에서 이동했던 방향과 속도를 기억하여, 현재 스텝의 이동 방향에 더해줍니다.
  - **장점**: 울퉁불퉁한 경사를 부드럽게 나아가도록 하여 **지역 최소점(Local Minimum)이나 안장점을 탈출**하는 데 도움을 줍니다.
  - **단점**: 관성 때문에 최적점을 지나쳐버리는 **오버슈팅(Overshooting)** 현상이 발생할 수 있습니다.
- **Nesterov Momentum (NAG)**: 모멘텀 방식의 오버슈팅을 완화하기 위한 기법입니다. 현재 위치가 아닌, **관성으로 한 스텝 미리 가본 위치**에서 기울기를 계산하여 이동 방향을 조절합니다. 마치 정찰병을 보내 진행 방향을 미리 확인하고 속도를 조절하는 것과 같습니다.

## 적응적 학습률: 똑똑한 보폭 조절

- **개념**: 모든 가중치에 동일한 학습률을 적용하는 대신, 각 가중치마다 개별적인 학습률을 적용하는 **적응적 학습률(Adaptive Learning Rate)** 기법입니다.
- **AdaGrad (Adaptive Gradient)**: **변화가 많았던 가중치는 학습률을 작게**, 변화가 적었던 가중치는 학습률을 크게 조절합니다.
  - **단점**: 학습이 길어질수록 모든 가중치의 학습률이 계속 작아져, 결국 학습이 멈추는 문제가 발생할 수 있습니다.
- **RMSProp (Root Mean Square Propagation)**: AdaGrad의 단점을 보완하기 위해, **최근 경로의 변화량**에만 집중하여 학습률을 조절합니다. 이를 통해 학습률이 0으로 수렴하는 문제를 방지합니다.

## Adam: 모멘텀과 적응적 학습률의 결합

- **Adam (Adaptive Moment Estimation)**: **모멘텀 방식과 RMSProp 방식의 장점을 결합**한 최적화 알고리즘입니다. 이동 방향과 보폭(학습률)을 모두 적응적으로 조절하여, 현재 가장 널리 사용되는 옵티마이저 중 하나입니다.
- **주의점**: Adam이 항상 최고의 성능을 보장하는 것은 아닙니다. 데이터셋이나 모델 구조에 따라 기본적인 SGD with Momentum이 더 좋은 성능을 보일 수도 있으므로, 맹신하지 않는 것이 중요합니다.
`;export{n as default};
