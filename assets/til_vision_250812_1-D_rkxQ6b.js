const n=`---
title: "안정적인 학습을 위한 시작점 설정: 가중치 초기화와 정규화"
date: "2025-08-12"
keywords: ["TIL", "딥러닝"]
---

# 안정적인 학습을 위한 시작점 설정: 가중치 초기화와 정규화

## 가중치 초기화 (Weight Initialization)

- **중요성**: 신경망 학습을 시작할 때 가중치를 어떤 값으로 설정하는지는 학습의 속도와 성능에 큰 영향을 미칩니다. 이는 손실 함수 공간의 어느 지점에서 경사 하강법을 시작할지를 결정하는 것과 같습니다.
- **대칭성 문제**: 모든 가중치를 동일한 값(예: 0 또는 1)으로 초기화하면, 모든 뉴런이 동일한 방식으로 업데이트되어 여러 개의 뉴런을 사용하는 의미가 사라집니다. 이를 **대칭성(Symmetry)** 문제라고 하며, 이를 피하기 위해 가중치는 보통 **무작위(Random)** 값으로 초기화합니다.

### 활성 함수에 따른 초기화 기법

- **핵심 아이디어**: 각 계층을 통과하더라도 데이터의 분산(크기)이 너무 커지거나 작아지지 않고 최대한 유지되도록 가중치를 초기화하는 것이 좋습니다.
- **Xavier(Glorot) 초기화**: 활성 함수가 **시그모이드(Sigmoid) 또는 하이퍼볼릭 탄젠트(tanh)** 계열일 때 주로 사용됩니다. 입력과 출력 노드의 개수를 고려하여 가중치의 분산을 조절합니다.
- **He 초기화**: 활성 함수가 **ReLU 계열**일 때 주로 사용됩니다. ReLU는 음수 입력을 0으로 만들기 때문에, 이 특성을 고려하여 Xavier 초기화 방식을 변형한 것입니다.

## 정규화 (Normalization)의 필요성

- **내부 공변량 변화 (Internal Covariate Shift)**: 딥러닝 학습 시, 각 계층을 통과하면서 데이터의 분포가 계속 바뀌는 현상을 말합니다. 앞쪽 계층의 가중치가 조금만 변해도 뒤쪽 계층의 입력 데이터 분포는 크게 달라지게 됩니다. 이로 인해 학습이 불안정해지고 **기울기 소실/폭주** 문제가 발생할 수 있습니다.
- **정규화란?**: 데이터의 분포를 특정 범위(예: 평균 0, 분산 1)로 일정하게 만들어주는 전반적인 기법을 의미합니다. 딥러닝에서는 내부 공변량 변화를 억제하여 학습을 안정화시키는 역할을 합니다.

## 배치 정규화 (Batch Normalization)

- **개념**: 신경망의 각 계층에 입력되는 **미니배치(mini-batch)** 데이터에 대해 매번 정규화를 수행하여, 데이터 분포를 일정하게 유지시키는 기법입니다.
- **알고리즘**:
  1. 미니배치의 평균과 분산을 계산합니다.
  2. 평균 0, 분산 1의 표준 정규분포로 데이터를 정규화합니다.
  3. 정규화된 데이터에 추가적인 학습 파라미터(scale, shift)를 적용하여, 원래 분포의 표현력을 잃지 않도록 복구합니다.
- **적용 위치**: 일반적으로 **가중치 합산(Wx+b)과 활성 함수(Activation) 사이**에 적용하는 것이 효과적이라고 알려져 있습니다.

### 배치 정규화의 장점

- **학습 안정화**: 내부 공변량 변화를 줄여주어 학습 과정을 매우 안정적으로 만듭니다.
- **높은 학습률 사용 가능**: 데이터 분포가 안정적이므로, 더 높은 학습률을 사용하여 학습 속도를 높일 수 있습니다.
- **초기화 의존도 감소**: 가중치 초기화에 대한 민감도를 줄여줍니다.
- **규제(Regularization) 효과**: 미니배치의 통계량을 사용하기 때문에 약간의 노이즈가 추가되는 효과가 있어, 과적합을 방지하는 데 도움을 줍니다.
`;export{n as default};
