const n=`---
title: "과적합 방지를 위한 규제(Regularization) 기법"
date: "2025-08-13"
keywords: ["TIL", "딥러닝"]
---

# 과적합 방지를 위한 규제(Regularization) 기법

- **일반화(Generalization)**: 모델이 훈련 데이터뿐만 아니라, 처음 보는 테스트 데이터에서도 좋은 성능을 보이는 것을 의미합니다.
- **과적합(Overfitting)**: 모델이 훈련 데이터에만 너무 치우쳐서 학습한 나머지, 훈련 데이터는 잘 맞추지만 테스트 데이터에서는 성능이 떨어지는 현상입니다.
- **규제(Regularization)**: 모델의 복잡도를 낮추어 과적합을 방지하고 일반화 성능을 높이는 모든 기법을 의미합니다.

## 가중치 감소 (Weight Decay): 가중치에 대한 패널티

- **개념**: 손실 함수에 가중치의 크기에 비례하는 항을 추가하여, 가중치가 너무 커지지 않도록 제한하는 기법입니다.
- **L2 규제 (Ridge)**: 가중치 제곱의 합(L2-norm)을 손실 함수에 더합니다. 가중치를 전반적으로 부드럽게(smooth) 작게 만들어 **스무딩** 효과를 줍니다. 이상치에 상대적으로 민감할 수 있습니다.
- **L1 규제 (Lasso)**: 가중치 절댓값의 합(L1-norm)을 손실 함수에 더합니다. 중요하지 않은 피처에 해당하는 가중치를 0으로 만들어 특정 피처를 완전히 제거하는 **샤프닝(피처 선택)** 효과를 줍니다. 이상치에 강건(robust)합니다.

## 조기 종료 (Early Stopping)

- **개념**: 과적합이 일어나기 전에 훈련을 멈추는 가장 간단하고 효과적인 규제 기법 중 하나입니다.
- **원리**: 훈련 과정에서 **검증 데이터(Validation data)**에 대한 성능을 지속적으로 확인합니다. 훈련 데이터에 대한 성능(손실)은 계속 좋아지지만, 검증 데이터에 대한 성능이 더 이상 개선되지 않고 정체되거나 나빠지기 시작하면 과적합의 신호로 보고 학습을 중단합니다.

## 데이터 증강 (Data Augmentation)

- **개념**: 과적합을 막는 가장 근본적인 방법은 **훈련 데이터의 양을 늘리는 것**입니다. 데이터 증강은 기존 훈련 데이터에 약간의 변형을 가하여, 원본 데이터와 유사하지만 새로운 데이터를 인위적으로 만들어내는 기법입니다.
- **기법**: 이미지의 경우 회전, 좌우 반전, 확대/축소, 잘라내기(crop), 밝기 조절 등의 방법을 사용합니다.
- **주의사항**:
  - **클래스 불변**: 데이터 증강 시 원본 데이터의 클래스(정답)가 바뀌지 않도록 주의해야 합니다.
  - **훈련 데이터에만 적용**: 데이터 증강은 **훈련 데이터에만 적용**해야 합니다. 검증/테스트 데이터에 적용하면 모델의 성능을 올바르게 평가할 수 없습니다.

## 드롭아웃 (Dropout)

- **개념**: 훈련 과정에서 각 뉴런을 **일정 확률로 무작위 비활성화**하는 기법입니다. 마치 매번 다른 구조의 신경망 여러 개를 학습시키는 것과 같은 효과를 줍니다.
- **원리**: 특정 뉴런에 의존하지 않고 여러 뉴런이 협력하여 특징을 학습하도록 강제하여, 모델의 복잡도를 낮추고 과적합을 방지합니다. 이는 여러 모델의 예측을 평균 내는 **앙상블(Ensemble)**과 유사한 효과를 냅니다.
- **적용**: 보통 입력층에는 0.8, 은닉층에는 0.5 정도의 확률로 뉴런을 유지(keep probability)하도록 설정하는 것을 권장합니다.
- **추론 단계**: 예측 시에는 모든 뉴런을 사용하되, 훈련 시 드롭아웃된 비율만큼 각 뉴런의 출력에 가중치를 곱하여 균형을 맞춥니다.

## 노이즈 주입 (Noise Injection)

- **개념**: 모델의 강건함(Robustness)을 높이기 위해 훈련 과정에서 의도적으로 노이즈를 주입하는 기법입니다.
- **종류**:
  - **입력 데이터에 노이즈 주입**: 입력 데이터(예: 이미지)에 가우시안 노이즈 등을 추가하여 데이터 증강 효과를 냅니다.
  - **가중치에 노이즈 주입**: 드롭아웃은 가중치에 노이즈를 주입하는 대표적인 방법 중 하나입니다.
- **효과**: 손실 함수 표면을 부드럽게 만들어 일반화 성능을 향상시키는 데 도움을 줍니다.
`;export{n as default};
