const n=`---
title: "CNN 아키텍처의 진화와 설계 원리"
date: "2025-08-26"
keywords: ["TIL", "CNN"]
---

# CNN 아키텍처의 진화와 설계 원리

딥러닝 모델의 정확도가 점차 상향 평준화되면서, 이제는 특정 목적에 맞는 효율적인 모델을 선택하고 설계하는 능력이 중요해졌습니다. 이는 마치 전자제품을 고를 때 성능, 크기, 전력 소모량 등을 종합적으로 고려하는 것과 같습니다.

## 초창기 CNN 모델: 기본 구조의 확립

- **LeNet-5**: 현대적인 CNN의 구조(합성곱 층 + 풀링 층 반복 -> 완전 연결 층)를 처음으로 제시한 선구적인 모델입니다.
- **AlexNet**: ImageNet 대회에서 압도적인 성능으로 우승하며 딥러닝의 시대를 연 모델입니다. **ReLU 활성 함수**, **드롭아웃**, **데이터 증강** 등의 기법을 성공적으로 도입했으며, 두 개의 GPU를 활용한 병렬 처리 구조를 설계했습니다.
- **ZFNet**: AlexNet의 구조를 **시각화(Visualization)** 기법을 통해 분석하고, 첫 번째 합성곱 층의 필터 크기와 스트라이드를 조절하여 성능을 개선했습니다.

## VGGNet: 깊이의 중요성 증명

- **핵심 아이디어**: 5x5나 7x7 같은 큰 필터 대신, **3x3의 작은 필터를 여러 겹 깊게 쌓는 것**이 더 효과적이라는 것을 증명했습니다.
- **장점**: 파라미터 수를 줄이면서도 더 깊은 네트워크를 통해 표현력을 높일 수 있었으며, 간단하고 균일한 구조 덕분에 다양한 문제에 특징 추출기(feature extractor)로 널리 활용됩니다.

## GoogLeNet (Inception): 효율적인 연산을 위한 설계

- **핵심 아이디어**: **인셉션 모듈(Inception Module)**을 통해, 하나의 입력에 대해 1x1, 3x3, 5x5 합성곱과 풀링을 병렬로 적용하고 그 결과를 합칩니다. 이는 네트워크가 스스로 최적의 특징을 다양한 관점에서 학습하도록 유도합니다.
- **병목 계층 (Bottleneck Layer)**: 인셉션 모듈의 연산량이 과도하게 증가하는 것을 막기 위해, 3x3이나 5x5 합성곱 이전에 **1x1 합성곱**을 두어 채널 수를 줄이는 병목 계층을 도입했습니다. 이는 연산량을 획기적으로 줄이면서도 성능을 유지하는 핵심적인 설계 기법이 되었습니다.

## ResNet: 깊이의 한계를 돌파한 혁신

- **문제점**: 네트워크가 무작정 깊어지면 오히려 성능이 떨어지는 **퇴화(Degradation)** 문제가 발생합니다.
- **핵심 아이디어**: **잔차 연결(Residual Connection)** 또는 **스킵 커넥션(Skip Connection)**을 도입했습니다. 이는 몇 개의 층을 건너뛰어 입력을 출력에 그대로 더해주는 구조입니다.
- **원리**: 이 구조를 통해 네트워크는 전체 출력을 새로 학습하는 대신, 입력과의 차이(변화량, 잔차)만을 학습하면 되므로 학습이 훨씬 쉬워집니다. 이를 통해 100층 이상의 매우 깊은 네트워크를 성공적으로 학습시킬 수 있게 되었습니다.
- **앙상블 효과**: ResNet을 깊게 쌓을수록 입력에서 출력까지 도달하는 경로가 지수적으로 늘어나, 마치 여러 모델을 동시에 학습시키는 **앙상블(Ensemble)**과 유사한 효과를 냅니다.

## ResNet 이후의 모델들: 너비, 분기, 자동화

ResNet의 성공 이후, 다양한 설계 사상을 가진 모델들이 등장했습니다.

- **Wide ResNet**: 네트워크를 깊게만 만드는 것에는 한계가 있으므로, **너비(채널 수)**를 늘리는 것이 더 효과적일 수 있다는 아이디어를 제시했습니다.
- **ResNeXt**: ResNet의 잔차 블록 내부 경로를 여러 개로 분기(Cardinality)하여, 다양한 특징을 더 효율적으로 학습하도록 설계했습니다.
- **DenseNet**: 각 층이 자신보다 앞에 있는 **모든 층과 직접 연결**되는 극단적인 형태의 연결 구조를 가집니다. 특징 맵을 계속 재사용하여 파라미터 효율성을 높입니다.
- **NASNet**: **신경망 구조 탐색(Neural Architecture Search, NAS)**이라는 AutoML 기법을 통해, 데이터셋에 가장 최적화된 네트워크 구조를 컴퓨터가 직접 찾도록 한 모델입니다.
`;export{n as default};
