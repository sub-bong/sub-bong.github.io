const n=`---
title: "딥러닝의 학습 원리: 역전파와 최적화"
date: "2025-08-07"
keywords: ["TIL", "딥러닝"]
---

# 딥러닝의 학습 원리: 역전파와 최적화

## 딥러닝 학습의 목표: 오차 최소화

딥러닝 모델의 학습은 **예측값과 실제 정답 간의 차이(오차, Error)를 최소화**하는 최적의 **가중치(w)와 편향(b)**을 찾는 과정입니다. 이 과정은 크게 두 단계로 나뉩니다.

1.  **순전파 (Forward Propagation)**: 입력 데이터가 입력층에서 출발하여 은닉층을 거쳐 출력층까지 이동하며 모델의 예측값을 계산하는 과정입니다.
2.  **역전파 (Backward Propagation)**: 순전파를 통해 계산된 예측값과 실제 정답 간의 오차를 계산하고, 이 오차를 출력층에서부터 입력층 방향으로 거꾸로 전파하며 각 계층의 가중치를 **수정(업데이트)**하는 과정입니다.

## 오차의 측정: 손실 함수 (Loss Function)

- **정의**: 모델의 예측이 얼마나 틀렸는지를 정량적으로 나타내는 함수입니다. **하나의 데이터**에 대한 오차를 계산합니다.
- **비용 함수 (Cost Function)**: **전체 훈련 데이터셋**에 대한 손실의 평균 또는 합계를 계산한 것입니다. 모델 학습은 이 비용 함수를 최소화하는 것을 목표로 합니다.
- **목적 함수 (Objective Function)**: 모델이 달성하고자 하는 최종 목표를 나타내는 함수로, 비용 함수를 포함하는 더 넓은 개념입니다.
- **종류**:
  - **MSE (Mean Squared Error)**: **회귀(Regression)** 문제에서 주로 사용. 오차의 제곱에 비례하여 큰 오차에 더 큰 패널티를 부여합니다.
  - **크로스 엔트로피 (Cross-Entropy)**: **분류(Classification)** 문제에서 주로 사용. 두 확률분포의 차이를 측정하여, 모델의 예측 확률분포가 실제 정답의 확률분포와 얼마나 다른지를 나타냅니다.

## 오차 수정 방법: 옵티마이저 (Optimizer)

- **역할**: 계산된 오차를 바탕으로, 비용 함수를 최소화하는 방향으로 각 뉴런의 가중치를 얼마나, 어떻게 업데이트할지를 결정하는 알고리즘입니다.
- **경사 하강법 (Gradient Descent)**: 가장 기본적인 최적화 방법. 비용 함수의 **기울기(Gradient, 미분값)**를 계산하여, 기울기가 가장 가파르게 감소하는 방향으로 가중치를 조금씩 이동시키는 방법입니다.
  - **핵심 원리**: "비용 함수를 최소화하려면, 현재 위치에서 기울기의 반대 방향으로 이동해야 한다."
  - **학습률 (Learning Rate)**: 이 때 한 번에 얼마나 이동할지, 즉 가중치를 얼마나 업데이트할지를 결정하는 **보폭(step size)**입니다. 학습률은 하이퍼파라미터로, 너무 크면 최적점을 지나쳐버리고, 너무 작으면 학습 속도가 매우 느려지거나 지역 최소점(Local Minimum)에 갇힐 수 있습니다.

## 역전파 (Backpropagation)의 핵심

- **정의**: **딥러닝의 핵심 알고리즘**으로, 출력층에서 계산된 오차를 기반으로 **미분의 연쇄 법칙(Chain Rule)**을 사용하여 각 은닉층과 입력층의 가중치가 오차에 얼마나 기여했는지를 계산하고, 그 기여도에 따라 가중치를 수정하는 방법입니다.
- **과정**: 출력층의 오차를 계산하고, 이 오차를 이용해 바로 이전 은닉층의 오차를 계산하고, 또 그 오차를 이용해 그 이전 은닉층의 오차를 계산하는 방식으로 입력층까지 오차를 역으로 전파하며 가중치를 업데이트합니다.

## 주요 학습 과제

- **기울기 소실 (Vanishing Gradient)**: 역전파 과정에서 기울기가 점점 작아져 0에 가까워지면서, 앞쪽 계층(입력층에 가까운)의 가중치가 거의 업데이트되지 않아 학습이 중단되는 현상입니다. 층이 매우 깊을 때 발생하기 쉽습니다.
- **지역 최소점 (Local Minimum)**: 비용 함수의 전체 최소점(Global Minimum)이 아닌, 특정 지역에서의 최소점에 빠져 더 이상 학습이 진행되지 않는 현상입니다. 다양한 옵티마이저(Momentum, Adam 등)는 이러한 문제를 완화하기 위해 제안되었습니다.
`;export{n as default};
