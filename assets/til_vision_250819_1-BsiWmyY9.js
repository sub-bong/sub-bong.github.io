const n=`---
title: "효율적인 딥러닝을 위한 전이 학습(Transfer Learning)"
date: "2025-08-19"
keywords: ["TIL", "전이 학습"]
---

# 효율적인 딥러닝을 위한 전이 학습(Transfer Learning)

## 전이 학습이란?

- **개념**: 특정 문제(Task A)를 해결하기 위해 대규모 데이터셋으로 학습된 **사전 훈련 모델(Pre-trained Model)**을, 관련된 새로운 문제(Task B)를 해결하는 데 재사용하는 기법입니다.
- **목표**: 처음부터 모델을 학습시키는 데 필요한 막대한 양의 데이터와 컴퓨팅 자원을 절약하고, 비교적 **적은 데이터**로도 높은 성능의 모델을 효율적으로 구축하는 것입니다.
- **핵심 아이디어**: 대규모 이미지 데이터(예: ImageNet)로 학습된 CNN 모델의 앞부분(합성곱 층)은 이미지의 보편적인 특징(선, 질감, 형태 등)을 추출하는 능력을 이미 갖추고 있습니다. 이 **특징 추출기(Feature Extractor)** 부분을 재사용하고, 해결하려는 특정 문제에 맞게 모델의 뒷부분(분류 층)만 새로 학습시키는 방식입니다.

## 전이 학습의 주요 방법

사전 훈련된 모델(예: VGG16, ResNet, EfficientNet)을 사용하는 방법은 크게 두 가지로 나뉩니다.

### 1. 특성 추출 (Feature Extraction)

- **개념**: 사전 훈련된 모델의 **합성곱 기반 층(Convolutional Base)은 그대로 두고(가중치 동결, freeze)**, 그 위에 새로운 **분류기 층(Classifier)**을 추가하여 이 새로운 분류기만 학습시키는 방법입니다.
- **과정**:
  1. 사전 훈련된 모델의 합성곱 층을 불러옵니다. 이 층들의 가중치는 학습되지 않도록 동결합니다.
  2. 그 위에 해결하려는 문제에 맞는 새로운 완전 연결 층(분류기)을 추가합니다.
  3. 새로운 데이터로 **새로운 분류기 부분만** 훈련시킵니다.
- **장점**: 학습해야 할 파라미터가 매우 적어 빠르고, 적은 데이터로도 과적합의 위험 없이 안정적으로 학습할 수 있습니다.

### 2. 미세 조정 (Fine-Tuning)

- **개념**: 특성 추출 방법에서 한 단계 더 나아가, 사전 훈련된 모델의 **상위 몇 개 층**의 동결을 해제하고, 아주 **작은 학습률(learning rate)**로 가중치를 미세하게 재조정하는 방법입니다.
- **과정**:
  1. 특성 추출과 동일한 방식으로 모델을 구성하고 새로운 분류기를 먼저 훈련시킵니다.
  2. 사전 훈련된 모델의 상위 계층 몇 개의 동결을 해제합니다.
  3. **매우 낮은 학습률**을 사용하여 모델 전체(동결 해제된 합성곱 층 + 새로운 분류기)를 다시 훈련시킵니다.
- **이유**: CNN의 하위 계층은 선, 질감 등 일반적이고 보편적인 특징을 학습하는 반면, 상위 계층은 좀 더 구체적이고 추상적인 특징(예: 개의 눈, 고양이의 귀)을 학습합니다. 미세 조정은 이러한 구체적인 특징을 새로운 데이터셋에 맞게 약간 수정하는 과정입니다.
- **주의사항**:
  - **낮은 학습률**: 반드시 매우 낮은 학습률을 사용해야 합니다. 학습률이 크면 사전 훈련된 모델이 학습한 유용한 특징들이 손상될 수 있습니다.
  - **과적합 위험**: 전체 계층을 미세 조정하거나, 데이터가 너무 적은 상태에서 미세 조정을 시도하면 과적합의 위험이 커집니다.

> **VGG16 모델**: 16개의 층으로 구성된 CNN 모델로, 구조가 간단하여 이해하고 사용하기 쉬워 전이 학습의 예시로 널리 사용됩니다. 최신 모델에 비해 성능이 약간 낮고 무겁지만, 여전히 강력한 특징 추출기로 활용됩니다.
`;export{n as default};
